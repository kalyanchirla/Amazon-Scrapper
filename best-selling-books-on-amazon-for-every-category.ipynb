{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Selling Books on Amazon Category wise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the steps I followed to build the project:\n",
    "\n",
    "- Selected https://www.amazon.co.uk/best-sellers-books-Amazon/zgbs/books/ref=zg_bs_nav_0 to scrape\n",
    "- I have found the list of all categories of books. For each category, I have stored the `CATEGORY_NAME`, `CATEGORY_PAGE_URL`\n",
    "- For each category, I have created a pandas data frame to store the `Book Name`, `Book URL`, `Author Name`, `Number of reviews`, `Rating`, `Format`, `Cheapest price in which it is available`.\n",
    "- For each category, I have created a csv file to store all the scraped information for the respective category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Scraping the list of book categories from Amazon\n",
    "\n",
    "For this, we are going to use:\n",
    "- `requests` to download the page and `bs4` to parse and extract information\n",
    "- We have to make a note that if `response.status_code` is not `200` indicates some error in loading the page. I have raised an exception for this down below. \\\n",
    "(More information on HTTP response status codes: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_home_page():\n",
    "    # The URL we wanted to parse \n",
    "    home_url = 'https://www.amazon.co.uk/best-sellers-books-Amazon/zgbs/books/ref=zg_bs_nav_0'\n",
    "    response = requests.get(home_url)\n",
    "    if response.status_code!=200:\n",
    "        raise Exception(f'Failed to load page {home_url}')\n",
    "    #Parsing the HTML Content of the web page\n",
    "    home_doc = BeautifulSoup(response.text,'html.parser')\n",
    "    return home_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_doc = get_home_page()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as we have got the home page in the variable `home_doc`, we have to identify the section of page that we would be needing to scrape the book categories\n",
    "\n",
    "I have selected all the children of the `div` element with id `zg-left-col` to get the entire categories section of the page. We can use the function `.findChildren()` to get all the children\n",
    "\n",
    "![](https://i.imgur.com/o3tyrRh.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are `73` children elements to the `div` element we have selected but we only need the first element out of these `73` children elements. I have selected all the `a` elements in the first child of the `div` section. There are exactly 34 such elements which correspond to all the 33 categories of books and also one extra element corresponding to `Any Department` which we do not need and can slice it down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories(home_doc):\n",
    "    #Getting the entire left section of the page into the variable 'categories_section'\n",
    "    selection_class='zg-left-col'\n",
    "    categories_section = home_doc.find('div',{'id':selection_class}).findChildren()\n",
    "    # Getting all the book categories\n",
    "    book_categories = categories_section[0].find_all('a')\n",
    "    # Slicing the 'Any Department' element from the list\n",
    "    book_categories = book_categories[1:]\n",
    "    return book_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_categories = get_categories(home_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(book_categories) #The length of book_categories should be 33 representing all the 33 categories of books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have noticed one category by name `Calendars, Diaries, Annuals & More` which contains only weekly planners, diaries. So they do not contain any `author` or `book_format`. We can exclude this category by slicing it out from our list using `book_categories[:3]+book_categories[4:]`\n",
    "\n",
    "I have not excluded this category and worked out for scraping this category too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names=[]\n",
    "category_urls=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have defined two lists by names `category_names` and `category_urls` to store the category names and their urls respectively by creating a helper function down below\n",
    "\n",
    "The name of the ith category can be found by `book_categories[i].text` and the url can be found by `book_categories[i]['href']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names_and_urls(book_categories):\n",
    "    # Iterating throught the book_categories list to get the category names and url\n",
    "    for category in range(len(book_categories)):\n",
    "        category_names.append(book_categories[category].text)\n",
    "        category_urls.append(book_categories[category]['href'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_names_and_urls(book_categories) #Running the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Art, Architecture & Photography', 'Biography', 'Business, Finance & Law', 'Calendars, Diaries, Annuals & More', \"Children's Books\"]\n",
      "\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "# Checking if the code is running as expected\n",
    "print(category_names[:5])\n",
    "print()\n",
    "print(len(category_urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes the part 1 which is to scrape the list of book categories from Amazon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Getting top 50 best selling books from each category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First we have to get the web page for each category given the category_url\n",
    "- I have created a helper function to do this down below\n",
    "\n",
    "Note:\n",
    "1. We have 33 category pages to get and while running, some pages may fail to load and result in `status_code!=200`\n",
    "2. For this problem, I have created a `Queue` which contains all the category_urls that are not yet visited\n",
    "3. We try to get the category_url page and if the `status_code!=200` then we push the category_url back to the queue\n",
    "4. I have also initialized a queue which contains all the unvisited_category_names. We push the category name back into this queue when we are pushing its corresponding url back into its queue i.e, when `status_code!=200`. \n",
    "5. This unvisited_category_names queue would be of use to name our `csv` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "unvisited_urls = Queue() #Initializing Queue to store all the unvisited urls\n",
    "unvisited_category_names = Queue() #Initializing Queue to store all the unvisited category names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_page(category_url):\n",
    "    #Getting the web page\n",
    "    response = requests.get(category_url)\n",
    "    # Checking the status code of the response\n",
    "    if response.status_code!=200:\n",
    "        print(f'Failed to load page {category_url}') \n",
    "        # I have used 'return 0' to make the calling function notice that there is a `status_code!=200` and the url should be added back to unvisited_queue\n",
    "        return 0 \n",
    "    # Parsing the web page using Beautiful Soup\n",
    "    category_doc = BeautifulSoup(response.text,'html.parser')\n",
    "    return category_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Comics-Graphic-Novels/zgbs/books/274081\n"
     ]
    }
   ],
   "source": [
    "#Checking whether the above function is working\n",
    "category_doc = get_category_page(category_urls[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, we have to get all the best selling books from this category\n",
    "- I have selected all the span classes with class `aok-inline-block zg-item` to get all the books in the page. There are exactly 50 span classes with this class corresponding to all the books in the respective category page\n",
    "\n",
    "This can be done by:\n",
    "`books = category_doc.find_all('span',{'class':'aok-inline-block zg-item'})`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By following the above steps, we will have all the 50 best selling books in the variable `books`\n",
    "\n",
    "The question is: \n",
    "How do we get the attributes:\n",
    "1. Name of Book\n",
    "2. Author of the book\n",
    "3. URL of the book\n",
    "4. Number of reviews for the book\n",
    "5. Current rating \n",
    "6. Format of the book (such as: hardcover, paperback, kindle edition, audio book)\n",
    "7. Price of the book\n",
    "\n",
    "for all the 50 books in the variable `books`\n",
    "\n",
    "Here is what I have found something interesting. `books[i]` contains information for ith book. All the above discussed attributes are contained in the children of `books[i]` as it is a `span` element. \n",
    "\n",
    "There are 4 classes of books we have to consider while scraping.\n",
    "\n",
    "1. When the length of `books[i].findChildren()` is `18`\n",
    "2. When the length of `books[i].findChildren()` is `16`\n",
    "3. When the length of `books[i].findChildren()` is `14`\n",
    "4. When the length of `books[i].findChildren()` is `13`\n",
    "\n",
    "The number of children for `books[i]` only represents the attributed that we discussed above. If the length of `books[i].findChildren() < 18` then it means that there are some missing attributes for that book\n",
    "\n",
    "Let us see some examples of each class:\n",
    "1. When the length of `books[i].findChildren()` is `18`\n",
    "![](https://i.imgur.com/iz5Emu2.png)\n",
    "\n",
    "The books with `number of children = 18` contains all the 7 attributes that we want without any missing attributes.<br/>\n",
    "\n",
    "2. When the length of `books[i].findChildren()` is `16`\n",
    "![](https://i.imgur.com/lF49cm5.png)\n",
    "\n",
    "The books with `number of children = 16` contains 1 missing attribute, `author_name`. Therefore,`author_name` should be `NULL` for these books\n",
    "\n",
    "3. When the length of `books[i].findChildren()` is `14`\n",
    "![](https://i.imgur.com/DuwFIQC.png)\n",
    "\n",
    "The book with `number of children = 14` contains 2 missing attributes: `author_name` and `book_format`. Therefore, `author_name` and `book_format` should be `NULL` for these books\n",
    "\n",
    "4. When the length of `books[i].findChildren()` is `13`\n",
    "![](https://i.imgur.com/Q6vdS3L.png)\n",
    "\n",
    "The book with `number of children = 13` contains 2 missing attributes: `number_of_reviews` and `rating`. Therefore, `number_of_reviews` and `rating` should be `NULL` for these books\n",
    "\n",
    "###### How do we find the attributes given the length of `.findChildren()` ?\n",
    "\n",
    "- This needs to be tested out manually. I have individually checked for each and every attribute for all the above mentioned cases.\n",
    "\n",
    "- For Example: Let `book = books[i].findChildren()`\n",
    "- `book[4]` contains the name of the book irrespective of `len(book)`.\n",
    "- `book[11]` contains the number of reviews when `len(book)==18`\n",
    "- `book[9]` contains the number of reviews when `len(book)==16`\n",
    "- `book[7]` contains the book format when `len(book)==13`\n",
    "\n",
    "- We have to manually find out all the cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Having all the information to get the attributes for the 50 best selling books in a respective page, we need to now create a helper function to do the following\n",
    "\n",
    "- We also need to import pandas to convert our category dictionary to a Data Frame and then to `.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing pandas \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing `category_doc` to scrape and `category_name` for naming the csv file after parsing the page\n",
    "def get_info_books(category_doc,category_name):\n",
    "    selection_class = 'aok-inline-block zg-item'\n",
    "    # `books` contains all the information associated with the 50 best selling books in the category page\n",
    "    books = category_doc.find_all('span',{'class':selection_class})\n",
    "    base_url = 'https://amazon.co.uk'\n",
    "    # Creating a dictionary to store the attributes in the current page\n",
    "    category_books_dict={\n",
    "        'book_name':[],\n",
    "        'book_author':[],\n",
    "        'book_URL':[],\n",
    "        'rating':[],\n",
    "        'number_of_reviews':[],\n",
    "        'book_format':[],\n",
    "        'price_of_book':[]\n",
    "    }\n",
    "    #The length of books is 50. So, we get exactly 50 best selling books\n",
    "    for i in range(len(books)):\n",
    "        # 'book' contains all the information associated with the ith book in the page\n",
    "        book = books[i].findChildren()\n",
    "        #I have used '.strip()' method to clear out extra spaces in the text. We can achieve our task without this.\n",
    "        if(len(book)==18): #No missing attributes\n",
    "            category_books_dict['book_name'].append(book[4].text.strip())\n",
    "            category_books_dict['book_author'].append(book[5].text.strip())\n",
    "            current_url = base_url+book[0]['href']\n",
    "            category_books_dict['book_URL'].append(current_url)\n",
    "            category_books_dict['rating'].append(book[9].text.strip())\n",
    "            category_books_dict['number_of_reviews'].append(book[11].text.strip())\n",
    "            category_books_dict['book_format'].append(book[12].text.strip())\n",
    "            category_books_dict['price_of_book'].append(book[17].text.strip())\n",
    "        elif(len(book)==16): #'author_name' is missing\n",
    "            category_books_dict['book_name'].append(book[4].text.strip())\n",
    "            category_books_dict['book_author'].append('NULL')\n",
    "            current_url = base_url+book[0]['href']\n",
    "            category_books_dict['book_URL'].append(current_url)\n",
    "            category_books_dict['rating'].append(book[8].text.strip())\n",
    "            category_books_dict['number_of_reviews'].append(book[9].text.strip())\n",
    "            category_books_dict['book_format'].append(book[11].text.strip())\n",
    "            category_books_dict['price_of_book'].append(book[15].text.strip())\n",
    "        elif(len(book)==14): # 'author_name' and 'book_format' are missing\n",
    "            category_books_dict['book_name'].append(book[4].text.strip())\n",
    "            category_books_dict['book_author'].append('NULL')\n",
    "            current_url = base_url+book[0]['href']\n",
    "            category_books_dict['book_URL'].append(current_url)\n",
    "            category_books_dict['rating'].append(book[8].text.strip())\n",
    "            category_books_dict['number_of_reviews'].append(book[9].text.strip())\n",
    "            category_books_dict['book_format'].append('NULL')\n",
    "            category_books_dict['price_of_book'].append(book[13].text.strip())\n",
    "        else: #len(book)==13. 'num_reviews' and 'rating' are missing\n",
    "            category_books_dict['book_name'].append(book[4].text.strip())\n",
    "            category_books_dict['book_author'].append(book[5].text.strip())\n",
    "            current_url = base_url+book[0]['href']\n",
    "            category_books_dict['book_URL'].append(current_url)\n",
    "            category_books_dict['rating'].append('NULL')\n",
    "            category_books_dict['number_of_reviews'].append('NULL')\n",
    "            category_books_dict['book_format'].append(book[7].text.strip())\n",
    "            category_books_dict['price_of_book'].append(book[12].text.strip())\n",
    "            \n",
    "        \n",
    "    #Converting the dictionary to a Data Frame using pandas \n",
    "    category_books_df = pd.DataFrame(category_books_dict)\n",
    "    #'path' variable specifies the name of the `csv` file that we would like to store all our information into\n",
    "    path = category_name+'.csv'\n",
    "    # Converting the dataframe to a csv file\n",
    "    category_books_df.to_csv(path,index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - We have to create a helper function to initialize the `unvisited_urls` queue and `unvisited_category_names` queue to carry out further steps\n",
    " \n",
    " - Initially, we can fill the queues with all the category_names and all the category_urls respectively as all of them are unvisited initially and all of them needs to be scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function to fill the queues with all the 33 category_urls and category_names\n",
    "def fill_queues():\n",
    "    for idx in range(len(category_urls)):\n",
    "        unvisited_urls.put(category_urls[idx])\n",
    "        unvisited_category_names.put(category_names[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the fill_queues function\n",
    "fill_queues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of unvisited_urls queue: 33 and size of unvisited_category_names queue: 33\n"
     ]
    }
   ],
   "source": [
    "# Checking if the queues are properly filled. The size of both queues should be `33`\n",
    "\n",
    "print(f'Size of unvisited_urls queue: {unvisited_urls.qsize()} and size of unvisited_category_names queue: {unvisited_category_names.qsize()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All the helper functions that we need are created now\n",
    "- The only part remaining is to create the final `scraping` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final scrape function which performs the entire thing for us\n",
    "def scrape_categories():\n",
    "    # Iterate until the queue become empty as there should not be any unvisited url\n",
    "    while unvisited_urls.empty()==False:\n",
    "        unvisited_url = unvisited_urls.get()\n",
    "        unvisited_cat_name = unvisited_category_names.get()\n",
    "        print(f'Scraping Category: {unvisited_url}')\n",
    "        category_doc = get_category_page(unvisited_url)\n",
    "        if category_doc == 0: # if category_doc==0 , it implies that the status_code!=200 for this page and we need to push this back to unvisited queue\n",
    "            print(f'Pushing the category: {unvisited_cat_name} to the unvisited queue')\n",
    "            unvisited_urls.put(unvisited_url)\n",
    "            #Also pushing the category_name to its corresponding unvisited queue\n",
    "            unvisited_category_names.put(unvisited_cat_name)\n",
    "        else:\n",
    "            # If the status_code==200 then we can parse the page and save the info in the 'category_name.csv' file\n",
    "            get_info_books(category_doc,unvisited_cat_name)\n",
    "    print(\"SCRAPING IS DONE!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Arts-Photography/zgbs/books/91\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Biographies-Memoirs/zgbs/books/67\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Business-Finance-Law/zgbs/books/68\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Calendars-Diaries-Annuals/zgbs/books/507848\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Childrens/zgbs/books/69\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Childrens/zgbs/books/69\n",
      "Pushing the category: Children's Books to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Comics-Graphic-Novels/zgbs/books/274081\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Comics-Graphic-Novels/zgbs/books/274081\n",
      "Pushing the category: Comics & Graphic Novels to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Computing-Internet/zgbs/books/71\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Computing-Internet/zgbs/books/71\n",
      "Pushing the category: Computers & Internet to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Crime-Thrillers-Mystery/zgbs/books/72\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Education-Studies-Teaching-Resources/zgbs/books/496792\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Erotica/zgbs/books/9587997031\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Fiction/zgbs/books/62\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Food-Drink/zgbs/books/66\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-LGBTQ/zgbs/books/275835\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Health-Family-Lifestyle/zgbs/books/74\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Health-Family-Lifestyle/zgbs/books/74\n",
      "Pushing the category: Health, Family & Lifestyle to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-History/zgbs/books/65\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-History/zgbs/books/65\n",
      "Pushing the category: History to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Home-Garden/zgbs/books/64\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Horror/zgbs/books/63\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Humor/zgbs/books/89\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Humor/zgbs/books/89\n",
      "Pushing the category: Humour to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Languages/zgbs/books/275738\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Languages/zgbs/books/275738\n",
      "Pushing the category: Languages to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Mind-Body-Spirit/zgbs/books/61\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Mind-Body-Spirit/zgbs/books/61\n",
      "Pushing the category: Mind, Body & Spirit to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Poetry-Drama-Criticism/zgbs/books/275389\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Poetry-Drama-Criticism/zgbs/books/275389\n",
      "Pushing the category: Poetry, Drama & Criticism to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Reference/zgbs/books/59\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Religion-Spirituality/zgbs/books/58\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Romance/zgbs/books/88\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Romance/zgbs/books/88\n",
      "Pushing the category: Romance to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-School/zgbs/books/5106747031\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Science-Nature-Maths/zgbs/books/57\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Science-Nature-Maths/zgbs/books/57\n",
      "Pushing the category: Science & Nature to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Science-Fiction-Fantasy/zgbs/books/4034595031\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Science-Fiction-Fantasy/zgbs/books/4034595031\n",
      "Pushing the category: Science Fiction & Fantasy to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Scientific-Technical-Medical/zgbs/books/564334\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Society-Politics-Philosophy/zgbs/books/60\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Society-Politics-Philosophy/zgbs/books/60\n",
      "Pushing the category: Society, Politics & Philosophy to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Sports-Hobbies-Games/zgbs/books/55\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Sports-Hobbies-Games/zgbs/books/55\n",
      "Pushing the category: Sports, Hobbies & Games to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Travel-Tourism/zgbs/books/83\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-University-Textbooks/zgbs/books/14909553031\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-University-Textbooks/zgbs/books/14909553031\n",
      "Pushing the category: University Textbooks to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Teen-Young-Adult/zgbs/books/52\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Teen-Young-Adult/zgbs/books/52\n",
      "Pushing the category: Young Adult to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Childrens/zgbs/books/69\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Childrens/zgbs/books/69\n",
      "Pushing the category: Children's Books to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Comics-Graphic-Novels/zgbs/books/274081\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Comics-Graphic-Novels/zgbs/books/274081\n",
      "Pushing the category: Comics & Graphic Novels to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Computing-Internet/zgbs/books/71\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Computing-Internet/zgbs/books/71\n",
      "Pushing the category: Computers & Internet to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Health-Family-Lifestyle/zgbs/books/74\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-History/zgbs/books/65\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Humor/zgbs/books/89\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Humor/zgbs/books/89\n",
      "Pushing the category: Humour to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Languages/zgbs/books/275738\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Languages/zgbs/books/275738\n",
      "Pushing the category: Languages to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Mind-Body-Spirit/zgbs/books/61\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Poetry-Drama-Criticism/zgbs/books/275389\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Poetry-Drama-Criticism/zgbs/books/275389\n",
      "Pushing the category: Poetry, Drama & Criticism to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Romance/zgbs/books/88\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Science-Nature-Maths/zgbs/books/57\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Science-Nature-Maths/zgbs/books/57\n",
      "Pushing the category: Science & Nature to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Science-Fiction-Fantasy/zgbs/books/4034595031\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Society-Politics-Philosophy/zgbs/books/60\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Sports-Hobbies-Games/zgbs/books/55\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Sports-Hobbies-Games/zgbs/books/55\n",
      "Pushing the category: Sports, Hobbies & Games to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-University-Textbooks/zgbs/books/14909553031\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Teen-Young-Adult/zgbs/books/52\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Teen-Young-Adult/zgbs/books/52\n",
      "Pushing the category: Young Adult to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Childrens/zgbs/books/69\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Childrens/zgbs/books/69\n",
      "Pushing the category: Children's Books to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Comics-Graphic-Novels/zgbs/books/274081\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Comics-Graphic-Novels/zgbs/books/274081\n",
      "Pushing the category: Comics & Graphic Novels to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Computing-Internet/zgbs/books/71\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Humor/zgbs/books/89\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Humor/zgbs/books/89\n",
      "Pushing the category: Humour to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Languages/zgbs/books/275738\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Poetry-Drama-Criticism/zgbs/books/275389\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Poetry-Drama-Criticism/zgbs/books/275389\n",
      "Pushing the category: Poetry, Drama & Criticism to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Science-Nature-Maths/zgbs/books/57\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Sports-Hobbies-Games/zgbs/books/55\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Sports-Hobbies-Games/zgbs/books/55\n",
      "Pushing the category: Sports, Hobbies & Games to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Teen-Young-Adult/zgbs/books/52\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Childrens/zgbs/books/69\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Comics-Graphic-Novels/zgbs/books/274081\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Comics-Graphic-Novels/zgbs/books/274081\n",
      "Pushing the category: Comics & Graphic Novels to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Humor/zgbs/books/89\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Humor/zgbs/books/89\n",
      "Pushing the category: Humour to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Poetry-Drama-Criticism/zgbs/books/275389\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Sports-Hobbies-Games/zgbs/books/55\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Sports-Hobbies-Games/zgbs/books/55\n",
      "Pushing the category: Sports, Hobbies & Games to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Comics-Graphic-Novels/zgbs/books/274081\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Humor/zgbs/books/89\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Humor/zgbs/books/89\n",
      "Pushing the category: Humour to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Sports-Hobbies-Games/zgbs/books/55\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Sports-Hobbies-Games/zgbs/books/55\n",
      "Pushing the category: Sports, Hobbies & Games to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Humor/zgbs/books/89\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Humor/zgbs/books/89\n",
      "Pushing the category: Humour to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Sports-Hobbies-Games/zgbs/books/55\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Humor/zgbs/books/89\n",
      "Failed to load page https://www.amazon.co.uk/Best-Sellers-Books-Humor/zgbs/books/89\n",
      "Pushing the category: Humour to the unvisited queue\n",
      "Scraping Category: https://www.amazon.co.uk/Best-Sellers-Books-Humor/zgbs/books/89\n",
      "SCRAPING IS DONE!!!!\n"
     ]
    }
   ],
   "source": [
    "# Calling the above defined scraping function\n",
    "scrape_categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References and Future Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
